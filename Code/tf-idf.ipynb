{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nos.chdir('/kaggle/input/')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-04T21:15:59.565030Z","iopub.execute_input":"2023-03-04T21:15:59.565482Z","iopub.status.idle":"2023-03-04T21:15:59.599349Z","shell.execute_reply.started":"2023-03-04T21:15:59.565391Z","shell.execute_reply":"2023-03-04T21:15:59.598570Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import fbeta_score\nfrom annoy import AnnoyIndex","metadata":{"execution":{"iopub.status.busy":"2023-03-04T21:16:09.291653Z","iopub.execute_input":"2023-03-04T21:16:09.291997Z","iopub.status.idle":"2023-03-04T21:16:09.936847Z","shell.execute_reply.started":"2023-03-04T21:16:09.291970Z","shell.execute_reply":"2023-03-04T21:16:09.935545Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"max_length = 512\ntop_n = 10\nmethod  = 'tf-idf' #tf-idf, count vectorizer, transformers","metadata":{"execution":{"iopub.status.busy":"2023-03-04T21:16:10.669093Z","iopub.execute_input":"2023-03-04T21:16:10.669494Z","iopub.status.idle":"2023-03-04T21:16:10.674303Z","shell.execute_reply.started":"2023-03-04T21:16:10.669461Z","shell.execute_reply":"2023-03-04T21:16:10.673069Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"if method == 'transformers':\n    import torch\n    from transformers import AutoTokenizer, AutoModel\n    %env TOKENIZERS_PARALLELISM=true\n\n    device = \"cuda\"\n    model_path = '/kaggle/input/paraphrasemultilingualmpnetbasev2'\n\n    model = AutoModel.from_pretrained(model_path)\n    model.eval()\n    model.to(device)\n\n    tokenizer = AutoTokenizer.from_pretrained(model_path)","metadata":{"execution":{"iopub.status.busy":"2023-03-04T21:16:12.877510Z","iopub.execute_input":"2023-03-04T21:16:12.877899Z","iopub.status.idle":"2023-03-04T21:16:12.885882Z","shell.execute_reply.started":"2023-03-04T21:16:12.877867Z","shell.execute_reply":"2023-03-04T21:16:12.884186Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Load the datasets\ntopics_df = pd.read_csv(\"learning-equality-curriculum-recommendations/topics.csv\")\ncontent_df = pd.read_csv(\"learning-equality-curriculum-recommendations/content.csv\")\ncorr_df = pd.read_csv(\"learning-equality-curriculum-recommendations/correlations.csv\")\nsubmission = pd.read_csv(\"learning-equality-curriculum-recommendations/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-03-04T21:16:14.036528Z","iopub.execute_input":"2023-03-04T21:16:14.036926Z","iopub.status.idle":"2023-03-04T21:16:34.446122Z","shell.execute_reply.started":"2023-03-04T21:16:14.036891Z","shell.execute_reply":"2023-03-04T21:16:34.444610Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"content_df.columns = ['content_'+ column for column in content_df.columns]","metadata":{"execution":{"iopub.status.busy":"2023-03-04T21:16:34.448248Z","iopub.execute_input":"2023-03-04T21:16:34.449104Z","iopub.status.idle":"2023-03-04T21:16:34.454638Z","shell.execute_reply.started":"2023-03-04T21:16:34.449049Z","shell.execute_reply":"2023-03-04T21:16:34.453366Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"corr_df['content_ids'] = corr_df['content_ids'].str.split()\ncorr_df = corr_df.explode('content_ids').reset_index(drop = True)\ncorr_df = corr_df.rename(columns = {'content_ids':'content_id'})\ncorr_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-04T21:16:34.456094Z","iopub.execute_input":"2023-03-04T21:16:34.456383Z","iopub.status.idle":"2023-03-04T21:16:34.786677Z","shell.execute_reply.started":"2023-03-04T21:16:34.456354Z","shell.execute_reply":"2023-03-04T21:16:34.785432Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"         topic_id      content_id\n0  t_00004da3a1b2  c_1108dd0c7a5d\n1  t_00004da3a1b2  c_376c5a8eb028\n2  t_00004da3a1b2  c_5bc0e1e2cba0\n3  t_00004da3a1b2  c_76231f9d0b5e\n4  t_00068291e9a4  c_639ea2ef9c95","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic_id</th>\n      <th>content_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>t_00004da3a1b2</td>\n      <td>c_1108dd0c7a5d</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>t_00004da3a1b2</td>\n      <td>c_376c5a8eb028</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>t_00004da3a1b2</td>\n      <td>c_5bc0e1e2cba0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>t_00004da3a1b2</td>\n      <td>c_76231f9d0b5e</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>t_00068291e9a4</td>\n      <td>c_639ea2ef9c95</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"topics_df_topic_tree = pd.DataFrame()\n\nfor channel in tqdm(topics_df['channel'].unique()):\n    channel_df = topics_df[(topics_df['channel'] == channel)].reset_index(drop = True)\n    for level in sorted(channel_df.level.unique()):\n        #For level 0, it first creates a topic tree column which is the title of that topic.            \n        if level == 0:\n            topic_tree = channel_df[channel_df['level'] == level]['title'].astype(str)\n            topic_tree_df = pd.DataFrame([channel_df[channel_df['level'] == level][['id']],topic_tree.values]).T\n            topic_tree_df.columns = ['child_id','topic_tree']\n            channel_df = channel_df.merge(topic_tree_df, left_on = 'id', right_on = 'child_id', how = 'left').drop(['child_id'], axis = 1)\n        \n        #Once the topic tree column has been created, the parent node and child node is merged on parent_id = child_id\n        topic_df_parent = channel_df[channel_df['level'] == level][['id','title','parent','topic_tree']]\n        topic_df_parent.columns = 'parent_' + topic_df_parent.columns\n        \n        topic_df_child = channel_df[channel_df['level'] == level + 1][['id','title','parent','topic_tree']]\n        topic_df_child.columns = 'child_' + topic_df_child.columns\n        \n        topic_df_merged = topic_df_parent.merge(topic_df_child, left_on = 'parent_id', right_on = 'child_parent')[['child_id','parent_id','parent_title','child_title','parent_topic_tree']]\n\n        #Topic tree is parent topic tree + title of the current child on that level\n        topic_tree = topic_df_merged['parent_topic_tree'].astype(str) + ' >> ' + topic_df_merged['child_title'].astype(str)\n        \n        topic_tree_df = pd.DataFrame([topic_df_merged['child_id'].values,topic_tree.values]).T\n        topic_tree_df.columns = ['child_id','topic_tree']\n        \n        channel_df = channel_df.merge(topic_tree_df, left_on = 'id', right_on = 'child_id', how = 'left').drop(['child_id'], axis = 1)\n        if 'topic_tree_y' in list(channel_df.columns):\n            channel_df['topic_tree'] = channel_df['topic_tree_x'].combine_first(channel_df['topic_tree_y'])\n            channel_df = channel_df.drop(['topic_tree_x','topic_tree_y'], axis = 1)\n        \n    topics_df_topic_tree = pd.concat([topics_df_topic_tree,channel_df])\n\ntopics_df_topic_tree = topics_df_topic_tree.reset_index(drop = True)\n\ntopics_df_topic_tree.columns = ['topic_'+ column for column in topics_df_topic_tree.columns]\ntopics_df_topic_tree = topics_df_topic_tree.rename(columns = {'topic_topic_tree':'topic_tree'})","metadata":{"execution":{"iopub.status.busy":"2023-03-04T21:16:34.788865Z","iopub.execute_input":"2023-03-04T21:16:34.789149Z","iopub.status.idle":"2023-03-04T21:16:49.643640Z","shell.execute_reply.started":"2023-03-04T21:16:34.789122Z","shell.execute_reply":"2023-03-04T21:16:49.642488Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"100%|██████████| 171/171 [00:14<00:00, 11.56it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"topics_df_topic_tree['topic_description'] = (topics_df_topic_tree['topic_description'].fillna('') + '[SEP]' + topics_df_topic_tree['topic_tree'].fillna(''))\ncontent_df['content_description'] = content_df['content_description'].fillna('') + content_df['content_text'].fillna('') + content_df['content_title'].fillna('')","metadata":{"execution":{"iopub.status.busy":"2023-03-04T21:16:49.645921Z","iopub.execute_input":"2023-03-04T21:16:49.646537Z","iopub.status.idle":"2023-03-04T21:16:50.806495Z","shell.execute_reply.started":"2023-03-04T21:16:49.646507Z","shell.execute_reply":"2023-03-04T21:16:50.805127Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"topics_df_topic_tree = topics_df_topic_tree[topics_df_topic_tree.topic_has_content].reset_index(drop = True)\ntopics_df_topic_tree = topics_df_topic_tree[topics_df_topic_tree['topic_language'] == 'en'].reset_index(drop = True)\ncontent_df = content_df[content_df['content_language'] == 'en'].reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2023-03-04T21:17:12.910964Z","iopub.execute_input":"2023-03-04T21:17:12.911323Z","iopub.status.idle":"2023-03-04T21:17:12.978594Z","shell.execute_reply.started":"2023-03-04T21:17:12.911291Z","shell.execute_reply":"2023-03-04T21:17:12.977748Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import re\ndef clean_text(text):\n    text = str(text).lower()\n    text = re.sub('\\[.*?@\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?@>+', '', text)\n    text = re.sub('\\n', ' ', text)\n    text = re.sub('\\@', '', text)\n    text = re.sub('\\_', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-03-04T21:17:14.863530Z","iopub.execute_input":"2023-03-04T21:17:14.863876Z","iopub.status.idle":"2023-03-04T21:17:14.871621Z","shell.execute_reply.started":"2023-03-04T21:17:14.863845Z","shell.execute_reply":"2023-03-04T21:17:14.870027Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"if method != 'transformers':\n    topics_df_topic_tree['topic_description_cleaned'] = topics_df_topic_tree['topic_description'].progress_apply(clean_text)\n    content_df['content_description_cleaned'] = content_df['content_description'].progress_apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2023-03-04T21:17:17.410631Z","iopub.execute_input":"2023-03-04T21:17:17.411842Z","iopub.status.idle":"2023-03-04T21:18:01.395993Z","shell.execute_reply.started":"2023-03-04T21:17:17.411781Z","shell.execute_reply":"2023-03-04T21:18:01.395006Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"100%|██████████| 28053/28053 [00:00<00:00, 35263.23it/s]\n100%|██████████| 65939/65939 [00:43<00:00, 1527.41it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nif method != 'transformers':\n    stop_words = stopwords.words('english')\n\n    def remove_stopwords(text):\n        text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n        return text\n\n    topics_df_topic_tree['topic_description_cleaned'] = topics_df_topic_tree['topic_description_cleaned'].progress_apply(remove_stopwords)\n    content_df['content_description_cleaned'] = content_df['content_description_cleaned'].progress_apply(remove_stopwords)\n    content_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-04T21:18:03.569810Z","iopub.execute_input":"2023-03-04T21:18:03.570775Z","iopub.status.idle":"2023-03-04T21:20:27.421033Z","shell.execute_reply.started":"2023-03-04T21:18:03.570717Z","shell.execute_reply":"2023-03-04T21:20:27.419998Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"100%|██████████| 28053/28053 [00:02<00:00, 13078.63it/s]\n100%|██████████| 65939/65939 [02:21<00:00, 466.94it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\n\nif method != 'transformers':\n    stemmer = nltk.SnowballStemmer(\"english\")\n\n    def stemm_text(text):\n        text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n        return text\n\n    topics_df_topic_tree['topic_description_cleaned'] = topics_df_topic_tree['topic_description_cleaned'].progress_apply(stemm_text)\n    content_df['content_description_cleaned'] = content_df['content_description_cleaned'].progress_apply(stemm_text)\n    content_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-04T21:20:30.122577Z","iopub.execute_input":"2023-03-04T21:20:30.122994Z","iopub.status.idle":"2023-03-04T21:27:51.915539Z","shell.execute_reply.started":"2023-03-04T21:20:30.122958Z","shell.execute_reply":"2023-03-04T21:27:51.913689Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"100%|██████████| 28053/28053 [00:07<00:00, 3943.38it/s]\n100%|██████████| 65939/65939 [07:14<00:00, 151.71it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_embeddings(list_text):\n    embeddings = []\n    \n    for text in tqdm(list_text):\n        tok = torch.tensor(tokenizer.encode(text, padding = 'max_length', truncation = True)).to(device).unsqueeze(0)\n        with torch.no_grad():\n            vec = model(tok).last_hidden_state.squeeze(0).mean(0).cpu().numpy()\n        embeddings.append(vec)\n        \n    return embeddings","metadata":{"execution":{"iopub.status.busy":"2023-03-04T21:27:51.918207Z","iopub.execute_input":"2023-03-04T21:27:51.918617Z","iopub.status.idle":"2023-03-04T21:27:51.926180Z","shell.execute_reply.started":"2023-03-04T21:27:51.918589Z","shell.execute_reply":"2023-03-04T21:27:51.924818Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nif method == 'tf-idf':\n    vectorizer = TfidfVectorizer()\n    vectorizer.fit(content_df['content_description_cleaned'])\n    content_vectors = vectorizer.transform(content_df['content_description_cleaned']).toarray()\n    topic_vectors = vectorizer.transform(topics_df_topic_tree['topic_description_cleaned']).toarray()\n    \nelif method == 'count vectorizer':\n    vectorizer = CountVectorizer()\n    vectorizer.fit(content_df['content_description_cleaned'])\n    content_vectors = vectorizer.transform(content_df['content_description_cleaned']).toarray()\n    topic_vectors = vectorizer.transform(topics_df_topic_tree['topic_description_cleaned']).toarray()\n\nelif method == 'transformers':\n    content_vectors = get_embeddings(list(content_df[\"content_description\"].values))\n    topic_vectors = get_embeddings(list(topics_df_topic_tree[\"topic_description\"].values))","metadata":{"execution":{"iopub.status.busy":"2023-03-04T21:27:51.927803Z","iopub.execute_input":"2023-03-04T21:27:51.928146Z","iopub.status.idle":"2023-03-04T21:28:48.131337Z","shell.execute_reply.started":"2023-03-04T21:27:51.928103Z","shell.execute_reply":"2023-03-04T21:28:48.127464Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"CPU times: user 46 s, sys: 10.1 s, total: 56.1 s\nWall time: 56.2 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nfrom scipy.sparse import csr_matrix\nfrom sklearn.decomposition import TruncatedSVD\n\nif method != 'transformers':\n    content_vectors = csr_matrix(content_vectors)\n    topic_vectors = csr_matrix(topic_vectors)\n    svd = TruncatedSVD(n_components = 10, random_state=42)\n    content_vectors = svd.fit_transform(content_vectors)\n    topic_vectors = svd.fit_transform(topic_vectors)","metadata":{"execution":{"iopub.status.busy":"2023-03-04T21:28:48.133578Z","iopub.execute_input":"2023-03-04T21:28:48.133906Z","iopub.status.idle":"2023-03-04T21:32:31.018530Z","shell.execute_reply.started":"2023-03-04T21:28:48.133871Z","shell.execute_reply":"2023-03-04T21:32:31.017449Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"CPU times: user 3min 27s, sys: 23.2 s, total: 3min 50s\nWall time: 3min 42s\n","output_type":"stream"}]},{"cell_type":"code","source":"nbrs = NearestNeighbors(n_neighbors = 50, metric = 'cosine').fit(content_vectors)","metadata":{"execution":{"iopub.status.busy":"2023-03-04T21:32:41.921778Z","iopub.execute_input":"2023-03-04T21:32:41.922171Z","iopub.status.idle":"2023-03-04T21:32:41.928933Z","shell.execute_reply.started":"2023-03-04T21:32:41.922137Z","shell.execute_reply":"2023-03-04T21:32:41.927782Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"topic_ids = list(topics_df_topic_tree['topic_id'].values)\ncontent_ids = list(content_df['content_id'].values)","metadata":{"execution":{"iopub.status.busy":"2023-03-04T21:32:43.049762Z","iopub.execute_input":"2023-03-04T21:32:43.050134Z","iopub.status.idle":"2023-03-04T21:32:43.058451Z","shell.execute_reply.started":"2023-03-04T21:32:43.050101Z","shell.execute_reply":"2023-03-04T21:32:43.057709Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# specify the target topic to use\ntopic_id = 't_06bdfbed8171'\ntopic_index = topics_df_topic_tree[topics_df_topic_tree['topic_id'] == topic_id].index.values[0]\ntlang = topics_df_topic_tree[topics_df_topic_tree['topic_id'] == topic_id]['topic_language'].values[0]\ntopic_tree = topics_df_topic_tree[topics_df_topic_tree['topic_id'] == topic_id]['topic_tree'].values[0]\n\ntopic_vector = topic_vectors[topic_index]\ndist, nb = nbrs.kneighbors([topic_vector])\n\n# get the set of ground truth content IDs correlated to the target topic\ntrue_content_ids = set(corr_df.loc[corr_df['topic_id'] == topic_id,'content_id'])\n\n# get the set of content IDs returned by the nearest neighbors model\n# (skipping over any content items where the language does not match)\npred_content_ids = []\nfor cindex in nb[0]:\n    cid = content_ids[cindex]\n    clang = content_df[content_df['content_id'] == cid]['content_language'].values[0]\n    if  clang == tlang:\n        pred_content_ids.append(cid)\n        \n# trim to only the top k results\nk = top_n\npred_content_ids = set(pred_content_ids[:k])\n\n# display the ground truth and predicted content item titles\nprint(\"True content:\")\nfor cid in true_content_ids:\n    content_title = content_df[content_df['content_id'] == cid].content_title.values[0]\n    print(\"  \", cid, \"\\t\", content_title)\n    \nprint(\"Predicted content:\")\nif pred_content_ids:\n    for cid in pred_content_ids:\n        content_title = content_df[content_df['content_id'] == cid].content_title.values[0]\n        print(\"  \", cid, \"\\t\", content_title)\n\n# calculate the confusion matrix variables\ntp = len(true_content_ids.intersection(pred_content_ids))\nfp = len(pred_content_ids - true_content_ids)\nfn = len(true_content_ids - pred_content_ids)\n\nprint(\"Ground truth count:\", len(true_content_ids))\nprint(\"Predicted count:\", len(pred_content_ids))\nprint(\"True positives:\", tp)\nprint(\"False positives:\", fp)\n\n# calculate the F2 score\nif tp or (fp and fn):\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    f2 = tp / (tp + 0.2 * fp + 0.8 * fn)\n    print(f\"F2: {round(f2*100,2)}%\")\n\nprint(f'Precision at {k}: {tp/k * 100}%')","metadata":{"execution":{"iopub.status.busy":"2023-03-04T21:32:54.673901Z","iopub.execute_input":"2023-03-04T21:32:54.674256Z","iopub.status.idle":"2023-03-04T21:32:55.066813Z","shell.execute_reply.started":"2023-03-04T21:32:54.674226Z","shell.execute_reply":"2023-03-04T21:32:55.065562Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"True content:\n   c_3a94b8b50c48 \t Chapter 1. Foundations\n   c_afab13ecb3d5 \t Chapter 2. Solving Linear Equations and Inequalities\n   c_c3cd764a6335 \t Chapter 6. Percents\nPredicted content:\n   c_78fafdb40be0 \t The Mean 2\n   c_1401ecd2e9de \t Level 2: The Mean\n   c_16a87e620109 \t Simulation showing value of t statistic\n   c_93242d63f027 \t The Mean of Means\n   c_a0dada46d589 \t Level 1: The Mean\n   c_9753c88fad23 \t 6.1 The meaning of locus\n   c_11dfc6a3bd5b \t Level 3: The Mean\n   c_7fb892d6095a \t Social theories overview (part 1)\n   c_a5855dc771b8 \t The Mean\n   c_e72e1a5d79f0 \t Mean\nGround truth count: 3\nPredicted count: 10\nTrue positives: 0\nFalse positives: 10\nF2: 0.0%\nPrecision at 10: 0.0%\n","output_type":"stream"}]},{"cell_type":"code","source":"# calculate the mean F2 over submission topics\nf2_scores = []\nprecision_at_k = []\navg_precision_at_k = []\n\nfor topic_id in tqdm(topic_ids[:1000]):\n    \n    topic_index = topics_df_topic_tree[topics_df_topic_tree['topic_id'] == topic_id].index.values[0]\n    tlang = topics_df_topic_tree[topics_df_topic_tree['topic_id'] == topic_id]['topic_language'].values[0]\n    topic_description = topics_df_topic_tree[topics_df_topic_tree['topic_id'] == topic_id]['topic_description'].values[0]\n    \n    topic_vector = topic_vectors[topic_index]\n\n    # calculate the nearest neighbors for the target topic\n    dist, nb = nbrs.kneighbors([topic_vector])\n\n    # get the set of ground truth content IDs correlated to the target topic\n    true_content_ids = set(corr_df.loc[corr_df['topic_id'] == topic_id,'content_id'])\n\n    # get the set of content IDs returned by the nearest neighbors model\n    # (skipping over any content items where the language does not match)\n    pred_content_ids = []\n    for cindex in nb[0]:\n        cid = content_ids[cindex]\n        clang = content_df[content_df['content_id'] == cid]['content_language'].values[0]\n        if clang == tlang:\n            pred_content_ids.append(cid)\n\n    # trim to only the top 20 results\n    pred_content_ids = set(pred_content_ids[:top_n])\n\n    # calculate the confusion matrix variables\n    tp = len(true_content_ids.intersection(pred_content_ids))\n    fp = len(pred_content_ids - true_content_ids)\n    fn = len(true_content_ids - pred_content_ids)\n\n    # calculate the F2 score\n    if len(true_content_ids) != 0:        \n        if pred_content_ids:\n            precision = tp / (tp + fp)\n            recall = tp / (tp + fn)\n            f2 = tp / (tp + 0.2 * fp + 0.8*fn)\n        else:\n            f2 = 0\n    else:\n        f2 = 0\n\n    f2_scores.append(f2)\n\n    precision_at_k.append(tp/k)\n    avg_precision_at_k.append(sum([tp/i for i in range(1,k+1)])/k)\n\nprint(\"Average F2:\", np.mean(f2_scores))\nprint(\"Average Precision at K:\", np.mean(avg_precision_at_k))","metadata":{"execution":{"iopub.status.busy":"2023-03-04T21:34:06.980308Z","iopub.execute_input":"2023-03-04T21:34:06.981337Z","iopub.status.idle":"2023-03-04T21:38:59.345040Z","shell.execute_reply.started":"2023-03-04T21:34:06.981275Z","shell.execute_reply":"2023-03-04T21:38:59.343583Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"100%|██████████| 1000/1000 [04:52<00:00,  3.42it/s]","output_type":"stream"},{"name":"stdout","text":"Average F2: 0.0\nAverage Precision at K: 0.0\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}